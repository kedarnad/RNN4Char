{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.Variable(3.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.GradientTape() as tape:\n",
    "    y=x**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "dy_dx = tape.gradient(y,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=tf.random.uniform(shape=[2,3])*0.01\n",
    "w=tf.random.uniform(shape=[2,1])\n",
    "b=tf.random.uniform(shape=[2,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.33529 3.53326821]\n",
      " [1.94069493 3.13867331]]\n"
     ]
    }
   ],
   "source": [
    "with tf.GradientTape() as tape:\n",
    "    y= tf.add(tf.multiply(x, tf.transpose(w)), b)\n",
    "    tf.print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.932971716 0.449030876 0.410639167]\n",
      " [0.20943439 0.924947739 0.284496427]]\n"
     ]
    }
   ],
   "source": [
    "y_hat = tf.random.uniform(shape=[2, 3])\n",
    "tf.print(y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "dy_dx =tape.gradient(y_hat,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(dy_dx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x tf.Tensor(\n",
      "[[1. 1.]\n",
      " [1. 1.]], shape=(2, 2), dtype=float32)\n",
      "\n",
      "y: tf.Tensor(4.0, shape=(), dtype=float32)\n",
      "\n",
      "z: tf.Tensor(16.0, shape=(), dtype=float32)\n",
      "\n",
      "dz_dx tf.Tensor(\n",
      "[[8. 8.]\n",
      " [8. 8.]], shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.ones((2, 2))\n",
    "print(\"x\", x)\n",
    "with tf.GradientTape() as t:\n",
    "    t.watch(x)\n",
    "    y=tf.reduce_sum(x)\n",
    "    print(\"\\ny:\",y)\n",
    "    z= tf.multiply(y, y)\n",
    "    print(\"\\nz:\", z)\n",
    "\n",
    "dz_dx = t.gradient(z, x)\n",
    "print(\"\\ndz_dx\", dz_dx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "dzx = dz_dx.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=tf.Variable(1.0)\n",
    "with tf.GradientTape() as t1:\n",
    "    with tf.GradientTape() as t2:\n",
    "        y=x*x*x\n",
    "#     dy_dx = t2.gradient(y, x)\n",
    "    \n",
    "# d2y_dx2 = t1.gradient(dy_dx , x)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert dy_dx.numpy() ==3.0\n",
    "# assert d2y_dx2.numpy()==6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.993307173]\n",
      "[0.00664805667]\n"
     ]
    }
   ],
   "source": [
    "#### using a sigmoid function for the derivative:\n",
    "x =tf.constant([5.0])\n",
    "with tf.GradientTape() as t:\n",
    "    t.watch(x)\n",
    "    y = tf.divide(1.0, tf.add(1.0, tf.exp(tf.negative(x))))\n",
    "\n",
    "tf.print(y)\n",
    "dy_dx= t.gradient(y, x)\n",
    "tf.print(dy_dx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "tf.print(dz_dx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for iteration # 0\n",
      "\n",
      "Input : \n",
      "[[0. 0. 1.]\n",
      " [0. 1. 1.]\n",
      " [1. 0. 1.]\n",
      " [1. 1. 1.]]\n",
      "Actual Output: \n",
      "[[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "Predicted Output: \n",
      "[[0.79266259]\n",
      " [0.81670564]\n",
      " [0.82386397]\n",
      " [0.8403057 ]]\n",
      "Loss: \n",
      "0.34976209628988036\n",
      "\n",
      "\n",
      "for iteration # 100\n",
      "\n",
      "Input : \n",
      "[[0. 0. 1.]\n",
      " [0. 1. 1.]\n",
      " [1. 0. 1.]\n",
      " [1. 1. 1.]]\n",
      "Actual Output: \n",
      "[[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "Predicted Output: \n",
      "[[0.49589323]\n",
      " [0.51073713]\n",
      " [0.50156393]\n",
      " [0.50880816]]\n",
      "Loss: \n",
      "0.2481531281556321\n",
      "\n",
      "\n",
      "for iteration # 200\n",
      "\n",
      "Input : \n",
      "[[0. 0. 1.]\n",
      " [0. 1. 1.]\n",
      " [1. 0. 1.]\n",
      " [1. 1. 1.]]\n",
      "Actual Output: \n",
      "[[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "Predicted Output: \n",
      "[[0.27319157]\n",
      " [0.62973051]\n",
      " [0.5750561 ]\n",
      " [0.55581184]]\n",
      "Loss: \n",
      "0.17530931162326968\n",
      "\n",
      "\n",
      "for iteration # 300\n",
      "\n",
      "Input : \n",
      "[[0. 0. 1.]\n",
      " [0. 1. 1.]\n",
      " [1. 0. 1.]\n",
      " [1. 1. 1.]]\n",
      "Actual Output: \n",
      "[[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "Predicted Output: \n",
      "[[0.1221552 ]\n",
      " [0.84479132]\n",
      " [0.81988486]\n",
      " [0.20146937]]\n",
      "Loss: \n",
      "0.028010748530356913\n",
      "\n",
      "\n",
      "for iteration # 400\n",
      "\n",
      "Input : \n",
      "[[0. 0. 1.]\n",
      " [0. 1. 1.]\n",
      " [1. 0. 1.]\n",
      " [1. 1. 1.]]\n",
      "Actual Output: \n",
      "[[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "Predicted Output: \n",
      "[[0.06807788]\n",
      " [0.92773403]\n",
      " [0.9042445 ]\n",
      " [0.0970421 ]]\n",
      "Loss: \n",
      "0.007110812698018953\n",
      "\n",
      "\n",
      "for iteration # 500\n",
      "\n",
      "Input : \n",
      "[[0. 0. 1.]\n",
      " [0. 1. 1.]\n",
      " [1. 0. 1.]\n",
      " [1. 1. 1.]]\n",
      "Actual Output: \n",
      "[[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "Predicted Output: \n",
      "[[0.0501434 ]\n",
      " [0.9512818 ]\n",
      " [0.93147553]\n",
      " [0.06665383]]\n",
      "Loss: \n",
      "0.00350654002622754\n",
      "\n",
      "\n",
      "for iteration # 600\n",
      "\n",
      "Input : \n",
      "[[0. 0. 1.]\n",
      " [0. 1. 1.]\n",
      " [1. 0. 1.]\n",
      " [1. 1. 1.]]\n",
      "Actual Output: \n",
      "[[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "Predicted Output: \n",
      "[[0.04121354]\n",
      " [0.96215033]\n",
      " [0.9448305 ]\n",
      " [0.05237209]]\n",
      "Loss: \n",
      "0.002229415687192885\n",
      "\n",
      "\n",
      "for iteration # 700\n",
      "\n",
      "Input : \n",
      "[[0. 0. 1.]\n",
      " [0. 1. 1.]\n",
      " [1. 0. 1.]\n",
      " [1. 1. 1.]]\n",
      "Actual Output: \n",
      "[[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "Predicted Output: \n",
      "[[0.0357403 ]\n",
      " [0.96848199]\n",
      " [0.95290011]\n",
      " [0.04395757]]\n",
      "Loss: \n",
      "0.0016053553834872408\n",
      "\n",
      "\n",
      "for iteration # 800\n",
      "\n",
      "Input : \n",
      "[[0. 0. 1.]\n",
      " [0. 1. 1.]\n",
      " [1. 0. 1.]\n",
      " [1. 1. 1.]]\n",
      "Actual Output: \n",
      "[[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "Predicted Output: \n",
      "[[0.03197589]\n",
      " [0.97267411]\n",
      " [0.95838087]\n",
      " [0.03834229]]\n",
      "Loss: \n",
      "0.0012428611424021446\n",
      "\n",
      "\n",
      "for iteration # 900\n",
      "\n",
      "Input : \n",
      "[[0. 0. 1.]\n",
      " [0. 1. 1.]\n",
      " [1. 0. 1.]\n",
      " [1. 1. 1.]]\n",
      "Actual Output: \n",
      "[[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "Predicted Output: \n",
      "[[0.02919272]\n",
      " [0.97568078]\n",
      " [0.96238867]\n",
      " [0.03429087]]\n",
      "Loss: \n",
      "0.0010085287377160318\n",
      "\n",
      "\n",
      "for iteration # 1000\n",
      "\n",
      "Input : \n",
      "[[0. 0. 1.]\n",
      " [0. 1. 1.]\n",
      " [1. 0. 1.]\n",
      " [1. 1. 1.]]\n",
      "Actual Output: \n",
      "[[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "Predicted Output: \n",
      "[[0.02703102]\n",
      " [0.97795795]\n",
      " [0.96547159]\n",
      " [0.03120785]]\n",
      "Loss: \n",
      "0.0008456671067012166\n",
      "\n",
      "\n",
      "for iteration # 1100\n",
      "\n",
      "Input : \n",
      "[[0. 0. 1.]\n",
      " [0. 1. 1.]\n",
      " [1. 0. 1.]\n",
      " [1. 1. 1.]]\n",
      "Actual Output: \n",
      "[[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "Predicted Output: \n",
      "[[0.02529099]\n",
      " [0.97975192]\n",
      " [0.96793194]\n",
      " [0.0287695 ]]\n",
      "Loss: \n",
      "0.0007264158595879086\n",
      "\n",
      "\n",
      "for iteration # 1200\n",
      "\n",
      "Input : \n",
      "[[0. 0. 1.]\n",
      " [0. 1. 1.]\n",
      " [1. 0. 1.]\n",
      " [1. 1. 1.]]\n",
      "Actual Output: \n",
      "[[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "Predicted Output: \n",
      "[[0.02385207]\n",
      " [0.98120792]\n",
      " [0.96995098]\n",
      " [0.02678391]]\n",
      "Loss: \n",
      "0.0006355963045684583\n",
      "\n",
      "\n",
      "for iteration # 1300\n",
      "\n",
      "Input : \n",
      "[[0. 0. 1.]\n",
      " [0. 1. 1.]\n",
      " [1. 0. 1.]\n",
      " [1. 1. 1.]]\n",
      "Actual Output: \n",
      "[[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "Predicted Output: \n",
      "[[0.02263678]\n",
      " [0.98241744]\n",
      " [0.97164446]\n",
      " [0.02512969]]\n",
      "Loss: \n",
      "0.0005642770076945529\n",
      "\n",
      "\n",
      "for iteration # 1400\n",
      "\n",
      "Input : \n",
      "[[0. 0. 1.]\n",
      " [0. 1. 1.]\n",
      " [1. 0. 1.]\n",
      " [1. 1. 1.]]\n",
      "Actual Output: \n",
      "[[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "Predicted Output: \n",
      "[[0.02159282]\n",
      " [0.98344113]\n",
      " [0.97309005]\n",
      " [0.02372604]]\n",
      "Loss: \n",
      "0.000506879113933541\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##### building an a simple ANN function ####\n",
    "\n",
    "\n",
    "import numpy as np \n",
    "      \n",
    "# # Each row is a training example, each column is a feature  [X1, X2, X3]\n",
    "# X=np.array(([0,0,1],[0,1,1],[1,0,1],[1,1,1]), dtype=float)\n",
    "# y=np.array(([0],[1],[1],[0]), dtype=float)\n",
    "\n",
    "# # Define useful functions    \n",
    "\n",
    "# # Activation function\n",
    "# def sigmoid(t):\n",
    "#     return 1/(1+np.exp(-t))\n",
    "\n",
    "# # Derivative of sigmoid\n",
    "# def sigmoid_derivative(p):\n",
    "#     return p * (1 - p)\n",
    "\n",
    "# # Class definition\n",
    "# class NeuralNetwork:\n",
    "#     def __init__(self, x,y):\n",
    "#         self.input = x\n",
    "#         self.weights1= np.random.rand(self.input.shape[1],4) # considering we have 4 nodes in the hidden layer\n",
    "#         self.weights2 = np.random.rand(4,1)\n",
    "#         self.y = y\n",
    "#         self.output = np. zeros(y.shape)\n",
    "        \n",
    "#     def feedforward(self):\n",
    "#         self.layer1 = sigmoid(np.dot(self.input, self.weights1))\n",
    "#         self.layer2 = sigmoid(np.dot(self.layer1, self.weights2))\n",
    "#         return self.layer2\n",
    "        \n",
    "#     def backprop(self):\n",
    "#         d_weights2 = np.dot(self.layer1.T, 2*(self.y -self.output)*sigmoid_derivative(self.output))\n",
    "#         d_weights1 = np.dot(self.input.T, np.dot(2*(self.y -self.output)*sigmoid_derivative(self.output), self.weights2.T)*sigmoid_derivative(self.layer1))\n",
    "    \n",
    "#         self.weights1 += d_weights1\n",
    "#         self.weights2 += d_weights2\n",
    "\n",
    "#     def train(self, X, y):\n",
    "#         self.output = self.feedforward()\n",
    "#         self.backprop()\n",
    "        \n",
    "\n",
    "# NN = NeuralNetwork(X,y)\n",
    "# for i in range(1500): # trains the NN 1,000 times\n",
    "#     if i % 100 ==0: \n",
    "#         print (\"for iteration # \" + str(i) + \"\\n\")\n",
    "#         print (\"Input : \\n\" + str(X))\n",
    "#         print (\"Actual Output: \\n\" + str(y))\n",
    "#         print (\"Predicted Output: \\n\" + str(NN.feedforward()))\n",
    "#         print (\"Loss: \\n\" + str(np.mean(np.square(y - NN.feedforward())))) # mean sum squared loss\n",
    "#         print (\"\\n\")\n",
    "  \n",
    "#     NN.train(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(t):\n",
    "    return 1/(1+np.exp(-t))\n",
    "\n",
    "def numpy_to_tensor(something):\n",
    "\n",
    "    something = tf.convert_to_tensor(something, dtype=tf.float32)\n",
    "    return something\n",
    "    \n",
    "Optim = tf.keras.optimizers.SGD(1e-1);\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, x,y):\n",
    "        self.input = numpy_to_tensor(x)  # tensor input\n",
    "        self.weights1= np.random.rand(self.input.shape[1],4) # considering we have 4 nodes in the hidden layer\n",
    "        self.weights2 = np.random.rand(4,1)\n",
    "        self.w1=numpy_to_tensor(self.weights1) # tensor weights\n",
    "        self.w2=numpy_to_tensor(self.weights2) # tensor weights\n",
    "        self.y = numpy_to_tensor(y)  # tensor output\n",
    "        self.output = np.zeros(y.shape)\n",
    "        self.output_tensor = numpy_to_tensor(self.output) # tensor output\n",
    "        \n",
    "        print(tf.shape(self.w1), tf.shape(self.w2))\n",
    "        \n",
    "    def train(self, iter):\n",
    "        \n",
    "        # feed forward using tensorflow:\n",
    "        \n",
    "        with tf.GradientTape() as t:\n",
    "            # taking the first layer dot product:\n",
    "            \n",
    "            t.watch([self.w1, self.w2])\n",
    "            self.first_dot_prd = tf.tensordot(self.input, self.w1, 1)\n",
    "            self.layer1 = tf.divide(1.0, tf.add(1.0, tf.exp(tf.negative(self.first_dot_prd))))\n",
    "\n",
    "            self.second_dot_prd = tf.tensordot(self.layer1, self.w2, 1)\n",
    "#             with tf.GradientTape() as t2:\n",
    "#             t2.watch(self.second_dot_prd)\n",
    "            # taking the second layer dot product:\n",
    "\n",
    "            self.layer2 = tf.divide(1.0, tf.add(1.0, tf.exp(tf.negative(self.second_dot_prd ))))\n",
    "            self.loss =tf.subtract(self.y, self.layer2)\n",
    "\n",
    "#                 tf.print(self.loss)\n",
    "            self.output=self.layer2\n",
    "\n",
    "        \n",
    "        loss=self.loss.numpy()\n",
    "        \n",
    "        if iter%2==0:\n",
    "            print(\"Loss: \"+str(np.square(np.mean(loss))))\n",
    "        \n",
    "        # doing a backward prop\n",
    "        \n",
    "        \n",
    "        grad = t.gradient(self.loss, [self.w1, self.w2])\n",
    "#         print(dw2)\n",
    "#         dw1 = t.gradient(self.second_dot_prd, self.w1)\n",
    "#         print(dw1)\n",
    "\n",
    "#         print(grad)\n",
    "        self.w1+=grad[0]\n",
    "        self.w2+=grad[1]\n",
    "#         Optim.apply_gradients(zip(grad,[self.w1,self.w2]));\n",
    "\n",
    "        \n",
    "            \n",
    "\n",
    "    \n",
    "            \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([3 4], shape=(2,), dtype=int32) tf.Tensor([4 1], shape=(2,), dtype=int32)\n",
      "Loss: 0.1365465\n",
      "Loss: 0.06692557\n",
      "Loss: 0.2171992\n",
      "Loss: 0.23092064\n",
      "Loss: 0.23654194\n"
     ]
    }
   ],
   "source": [
    "            \n",
    "X=np.array(([0,0,1],[0,1,1],[1,0,1],[1,1,1]), dtype=float)\n",
    "y=np.array(([0],[1],[1],[0]), dtype=float)\n",
    "            \n",
    "NN = NeuralNetwork(X, y)\n",
    "for i in range(10):\n",
    "    NN.train(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 1]\n",
      " [1 1 1 1]\n",
      " [1 1 1 1]] [[2 2 2]\n",
      " [2 2 2]\n",
      " [2 2 2]\n",
      " [2 2 2]]\n",
      "[[8 8 8]\n",
      " [8 8 8]\n",
      " [8 8 8]]\n"
     ]
    }
   ],
   "source": [
    "# x=numpy_to_tensor(np.array(([0,0,1],[0,1,1],[1,0,1],[1,1,1]), dtype=float))\n",
    "\n",
    "x= tf.ones((3, 4))\n",
    "\n",
    "w1=numpy_to_tensor(np.random.rand(3,4))\n",
    "y=tf.ones((4, 3))*2\n",
    "\n",
    "tf.print(x, y)\n",
    "\n",
    "tf.print(tf.tensordot(x, y, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
       "array([[0.        , 0.        , 0.02300561],\n",
       "       [0.        , 0.31643727, 0.3550229 ],\n",
       "       [0.7240439 , 0.        , 0.07971293],\n",
       "       [0.36281088, 0.07566915, 0.12640192]], dtype=float32)>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.multiply(x, tf.transpose(w1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
