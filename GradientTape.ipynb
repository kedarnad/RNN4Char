{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.Variable(3.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.GradientTape() as tape:\n",
    "    y=x**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "dy_dx = tape.gradient(y,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=tf.random.uniform(shape=[2,3])*0.01\n",
    "w=tf.random.uniform(shape=[2,1])\n",
    "b=tf.random.uniform(shape=[2,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.33529 3.53326821]\n",
      " [1.94069493 3.13867331]]\n"
     ]
    }
   ],
   "source": [
    "with tf.GradientTape() as tape:\n",
    "    y= tf.add(tf.multiply(x, tf.transpose(w)), b)\n",
    "    tf.print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.932971716 0.449030876 0.410639167]\n",
      " [0.20943439 0.924947739 0.284496427]]\n"
     ]
    }
   ],
   "source": [
    "y_hat = tf.random.uniform(shape=[2, 3])\n",
    "tf.print(y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "dy_dx =tape.gradient(y_hat,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(dy_dx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x tf.Tensor(\n",
      "[[1. 1.]\n",
      " [1. 1.]], shape=(2, 2), dtype=float32)\n",
      "\n",
      "y: tf.Tensor(4.0, shape=(), dtype=float32)\n",
      "\n",
      "z: tf.Tensor(16.0, shape=(), dtype=float32)\n",
      "\n",
      "dz_dx tf.Tensor(\n",
      "[[8. 8.]\n",
      " [8. 8.]], shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.ones((2, 2))\n",
    "print(\"x\", x)\n",
    "with tf.GradientTape() as t:\n",
    "    t.watch(x)\n",
    "    y=tf.reduce_sum(x)\n",
    "    print(\"\\ny:\",y)\n",
    "    z= tf.multiply(y, y)\n",
    "    print(\"\\nz:\", z)\n",
    "\n",
    "dz_dx = t.gradient(z, x)\n",
    "print(\"\\ndz_dx\", dz_dx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "dzx = dz_dx.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=tf.Variable(1.0)\n",
    "with tf.GradientTape() as t1:\n",
    "    with tf.GradientTape() as t2:\n",
    "        y=x*x*x\n",
    "#     dy_dx = t2.gradient(y, x)\n",
    "    \n",
    "# d2y_dx2 = t1.gradient(dy_dx , x)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert dy_dx.numpy() ==3.0\n",
    "# assert d2y_dx2.numpy()==6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.993307173]\n",
      "[0.00664805667]\n"
     ]
    }
   ],
   "source": [
    "#### using a sigmoid function for the derivative:\n",
    "x =tf.constant([5.0])\n",
    "with tf.GradientTape() as t:\n",
    "    t.watch(x)\n",
    "    y = tf.divide(1.0, tf.add(1.0, tf.exp(tf.negative(x))))\n",
    "\n",
    "tf.print(y)\n",
    "dy_dx= t.gradient(y, x)\n",
    "tf.print(dy_dx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "tf.print(dz_dx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for iteration # 0\n",
      "\n",
      "Input : \n",
      "[[0. 0. 1.]\n",
      " [0. 1. 1.]\n",
      " [1. 0. 1.]\n",
      " [1. 1. 1.]]\n",
      "Actual Output: \n",
      "[[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "Predicted Output: \n",
      "[[0.79266259]\n",
      " [0.81670564]\n",
      " [0.82386397]\n",
      " [0.8403057 ]]\n",
      "Loss: \n",
      "0.34976209628988036\n",
      "\n",
      "\n",
      "for iteration # 100\n",
      "\n",
      "Input : \n",
      "[[0. 0. 1.]\n",
      " [0. 1. 1.]\n",
      " [1. 0. 1.]\n",
      " [1. 1. 1.]]\n",
      "Actual Output: \n",
      "[[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "Predicted Output: \n",
      "[[0.49589323]\n",
      " [0.51073713]\n",
      " [0.50156393]\n",
      " [0.50880816]]\n",
      "Loss: \n",
      "0.2481531281556321\n",
      "\n",
      "\n",
      "for iteration # 200\n",
      "\n",
      "Input : \n",
      "[[0. 0. 1.]\n",
      " [0. 1. 1.]\n",
      " [1. 0. 1.]\n",
      " [1. 1. 1.]]\n",
      "Actual Output: \n",
      "[[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "Predicted Output: \n",
      "[[0.27319157]\n",
      " [0.62973051]\n",
      " [0.5750561 ]\n",
      " [0.55581184]]\n",
      "Loss: \n",
      "0.17530931162326968\n",
      "\n",
      "\n",
      "for iteration # 300\n",
      "\n",
      "Input : \n",
      "[[0. 0. 1.]\n",
      " [0. 1. 1.]\n",
      " [1. 0. 1.]\n",
      " [1. 1. 1.]]\n",
      "Actual Output: \n",
      "[[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "Predicted Output: \n",
      "[[0.1221552 ]\n",
      " [0.84479132]\n",
      " [0.81988486]\n",
      " [0.20146937]]\n",
      "Loss: \n",
      "0.028010748530356913\n",
      "\n",
      "\n",
      "for iteration # 400\n",
      "\n",
      "Input : \n",
      "[[0. 0. 1.]\n",
      " [0. 1. 1.]\n",
      " [1. 0. 1.]\n",
      " [1. 1. 1.]]\n",
      "Actual Output: \n",
      "[[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "Predicted Output: \n",
      "[[0.06807788]\n",
      " [0.92773403]\n",
      " [0.9042445 ]\n",
      " [0.0970421 ]]\n",
      "Loss: \n",
      "0.007110812698018953\n",
      "\n",
      "\n",
      "for iteration # 500\n",
      "\n",
      "Input : \n",
      "[[0. 0. 1.]\n",
      " [0. 1. 1.]\n",
      " [1. 0. 1.]\n",
      " [1. 1. 1.]]\n",
      "Actual Output: \n",
      "[[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "Predicted Output: \n",
      "[[0.0501434 ]\n",
      " [0.9512818 ]\n",
      " [0.93147553]\n",
      " [0.06665383]]\n",
      "Loss: \n",
      "0.00350654002622754\n",
      "\n",
      "\n",
      "for iteration # 600\n",
      "\n",
      "Input : \n",
      "[[0. 0. 1.]\n",
      " [0. 1. 1.]\n",
      " [1. 0. 1.]\n",
      " [1. 1. 1.]]\n",
      "Actual Output: \n",
      "[[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "Predicted Output: \n",
      "[[0.04121354]\n",
      " [0.96215033]\n",
      " [0.9448305 ]\n",
      " [0.05237209]]\n",
      "Loss: \n",
      "0.002229415687192885\n",
      "\n",
      "\n",
      "for iteration # 700\n",
      "\n",
      "Input : \n",
      "[[0. 0. 1.]\n",
      " [0. 1. 1.]\n",
      " [1. 0. 1.]\n",
      " [1. 1. 1.]]\n",
      "Actual Output: \n",
      "[[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "Predicted Output: \n",
      "[[0.0357403 ]\n",
      " [0.96848199]\n",
      " [0.95290011]\n",
      " [0.04395757]]\n",
      "Loss: \n",
      "0.0016053553834872408\n",
      "\n",
      "\n",
      "for iteration # 800\n",
      "\n",
      "Input : \n",
      "[[0. 0. 1.]\n",
      " [0. 1. 1.]\n",
      " [1. 0. 1.]\n",
      " [1. 1. 1.]]\n",
      "Actual Output: \n",
      "[[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "Predicted Output: \n",
      "[[0.03197589]\n",
      " [0.97267411]\n",
      " [0.95838087]\n",
      " [0.03834229]]\n",
      "Loss: \n",
      "0.0012428611424021446\n",
      "\n",
      "\n",
      "for iteration # 900\n",
      "\n",
      "Input : \n",
      "[[0. 0. 1.]\n",
      " [0. 1. 1.]\n",
      " [1. 0. 1.]\n",
      " [1. 1. 1.]]\n",
      "Actual Output: \n",
      "[[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "Predicted Output: \n",
      "[[0.02919272]\n",
      " [0.97568078]\n",
      " [0.96238867]\n",
      " [0.03429087]]\n",
      "Loss: \n",
      "0.0010085287377160318\n",
      "\n",
      "\n",
      "for iteration # 1000\n",
      "\n",
      "Input : \n",
      "[[0. 0. 1.]\n",
      " [0. 1. 1.]\n",
      " [1. 0. 1.]\n",
      " [1. 1. 1.]]\n",
      "Actual Output: \n",
      "[[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "Predicted Output: \n",
      "[[0.02703102]\n",
      " [0.97795795]\n",
      " [0.96547159]\n",
      " [0.03120785]]\n",
      "Loss: \n",
      "0.0008456671067012166\n",
      "\n",
      "\n",
      "for iteration # 1100\n",
      "\n",
      "Input : \n",
      "[[0. 0. 1.]\n",
      " [0. 1. 1.]\n",
      " [1. 0. 1.]\n",
      " [1. 1. 1.]]\n",
      "Actual Output: \n",
      "[[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "Predicted Output: \n",
      "[[0.02529099]\n",
      " [0.97975192]\n",
      " [0.96793194]\n",
      " [0.0287695 ]]\n",
      "Loss: \n",
      "0.0007264158595879086\n",
      "\n",
      "\n",
      "for iteration # 1200\n",
      "\n",
      "Input : \n",
      "[[0. 0. 1.]\n",
      " [0. 1. 1.]\n",
      " [1. 0. 1.]\n",
      " [1. 1. 1.]]\n",
      "Actual Output: \n",
      "[[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "Predicted Output: \n",
      "[[0.02385207]\n",
      " [0.98120792]\n",
      " [0.96995098]\n",
      " [0.02678391]]\n",
      "Loss: \n",
      "0.0006355963045684583\n",
      "\n",
      "\n",
      "for iteration # 1300\n",
      "\n",
      "Input : \n",
      "[[0. 0. 1.]\n",
      " [0. 1. 1.]\n",
      " [1. 0. 1.]\n",
      " [1. 1. 1.]]\n",
      "Actual Output: \n",
      "[[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "Predicted Output: \n",
      "[[0.02263678]\n",
      " [0.98241744]\n",
      " [0.97164446]\n",
      " [0.02512969]]\n",
      "Loss: \n",
      "0.0005642770076945529\n",
      "\n",
      "\n",
      "for iteration # 1400\n",
      "\n",
      "Input : \n",
      "[[0. 0. 1.]\n",
      " [0. 1. 1.]\n",
      " [1. 0. 1.]\n",
      " [1. 1. 1.]]\n",
      "Actual Output: \n",
      "[[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "Predicted Output: \n",
      "[[0.02159282]\n",
      " [0.98344113]\n",
      " [0.97309005]\n",
      " [0.02372604]]\n",
      "Loss: \n",
      "0.000506879113933541\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##### building an a simple ANN function ####\n",
    "\n",
    "\n",
    "import numpy as np \n",
    "      \n",
    "# # Each row is a training example, each column is a feature  [X1, X2, X3]\n",
    "# X=np.array(([0,0,1],[0,1,1],[1,0,1],[1,1,1]), dtype=float)\n",
    "# y=np.array(([0],[1],[1],[0]), dtype=float)\n",
    "\n",
    "# # Define useful functions    \n",
    "\n",
    "# # Activation function\n",
    "# def sigmoid(t):\n",
    "#     return 1/(1+np.exp(-t))\n",
    "\n",
    "# # Derivative of sigmoid\n",
    "# def sigmoid_derivative(p):\n",
    "#     return p * (1 - p)\n",
    "\n",
    "# # Class definition\n",
    "# class NeuralNetwork:\n",
    "#     def __init__(self, x,y):\n",
    "#         self.input = x\n",
    "#         self.weights1= np.random.rand(self.input.shape[1],4) # considering we have 4 nodes in the hidden layer\n",
    "#         self.weights2 = np.random.rand(4,1)\n",
    "#         self.y = y\n",
    "#         self.output = np. zeros(y.shape)\n",
    "        \n",
    "#     def feedforward(self):\n",
    "#         self.layer1 = sigmoid(np.dot(self.input, self.weights1))\n",
    "#         self.layer2 = sigmoid(np.dot(self.layer1, self.weights2))\n",
    "#         return self.layer2\n",
    "        \n",
    "#     def backprop(self):\n",
    "#         d_weights2 = np.dot(self.layer1.T, 2*(self.y -self.output)*sigmoid_derivative(self.output))\n",
    "#         d_weights1 = np.dot(self.input.T, np.dot(2*(self.y -self.output)*sigmoid_derivative(self.output), self.weights2.T)*sigmoid_derivative(self.layer1))\n",
    "    \n",
    "#         self.weights1 += d_weights1\n",
    "#         self.weights2 += d_weights2\n",
    "\n",
    "#     def train(self, X, y):\n",
    "#         self.output = self.feedforward()\n",
    "#         self.backprop()\n",
    "        \n",
    "\n",
    "# NN = NeuralNetwork(X,y)\n",
    "# for i in range(1500): # trains the NN 1,000 times\n",
    "#     if i % 100 ==0: \n",
    "#         print (\"for iteration # \" + str(i) + \"\\n\")\n",
    "#         print (\"Input : \\n\" + str(X))\n",
    "#         print (\"Actual Output: \\n\" + str(y))\n",
    "#         print (\"Predicted Output: \\n\" + str(NN.feedforward()))\n",
    "#         print (\"Loss: \\n\" + str(np.mean(np.square(y - NN.feedforward())))) # mean sum squared loss\n",
    "#         print (\"\\n\")\n",
    "  \n",
    "#     NN.train(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(t):\n",
    "    return 1/(1+np.exp(-t))\n",
    "\n",
    "def numpy_to_tensor(something):\n",
    "\n",
    "    something = tf.convert_to_tensor(something, dtype=tf.float32)\n",
    "    return something\n",
    "    \n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, x,y):\n",
    "        self.input = numpy_to_tensor(x)  # tensor input\n",
    "        self.weights1= np.random.rand(self.input.shape[1],4) # considering we have 4 nodes in the hidden layer\n",
    "        self.weights2 = np.random.rand(4,1)\n",
    "        self.w1=tf.Variable(numpy_to_tensor(self.weights1)) # tensor weights\n",
    "        self.w2=tf.Variable(numpy_to_tensor(self.weights2)) # tensor weights\n",
    "        self.y = numpy_to_tensor(y)  # tensor output\n",
    "        self.output = np.zeros(y.shape)\n",
    "        self.output_tensor = numpy_to_tensor(self.output) # tensor output\n",
    "        \n",
    "        print(tf.shape(self.w1), tf.shape(self.w2))\n",
    "        \n",
    "    def train(self, iter):\n",
    "        \n",
    "        # feed forward using tensorflow:\n",
    "        \n",
    "        with tf.GradientTape() as t:\n",
    "            # taking the first layer dot product:\n",
    "            \n",
    "            t.watch([self.w1, self.w2])\n",
    "            self.first_dot_prd = tf.tensordot(self.input, self.w1, 1)\n",
    "            self.layer1 = tf.divide(1.0, tf.add(1.0, tf.exp(tf.negative(self.first_dot_prd))))\n",
    "\n",
    "            self.second_dot_prd = tf.tensordot(self.layer1, self.w2, 1)\n",
    "#             with tf.GradientTape() as t2:\n",
    "#             t2.watch(self.second_dot_prd)\n",
    "            # taking the second layer dot product:\n",
    "\n",
    "            self.layer2 = tf.divide(1.0, tf.add(1.0, tf.exp(tf.negative(self.second_dot_prd ))))\n",
    "            self.loss = self.y-self.layer2   #tf.subtract(self.y, self.layer2)\n",
    "            \n",
    "            self.reduced_loss = tf.reduce_sum(tf.square(self.y - self.layer2))\n",
    "\n",
    "#                 tf.print(self.loss)\n",
    "            self.output=self.layer2\n",
    "\n",
    "        \n",
    "        loss=self.loss.numpy()\n",
    "        \n",
    "        if iter%5==0:\n",
    "            print(\"Loss: \"+str(np.square(np.mean(loss))))\n",
    "        \n",
    "        # doing a backward prop\n",
    "        \n",
    "        \n",
    "        grad = t.gradient(self.reduced_loss, [self.w1, self.w2])\n",
    "        \n",
    "#         grad = tf.distribute.get_replica_context().all_reduce('sum', grad)\n",
    "#         print(dw2)\n",
    "#         dw1 = t.gradient(self.second_dot_prd, self.w1)\n",
    "#         print(dw1)\n",
    "\n",
    "#         print(grad)\n",
    "#         print(self.w1, self.w2)\n",
    "#         self.w1+=1e-1*grad[0]\n",
    "#         self.w2+=1e-1*grad[1]\n",
    "#         print(grad[0], grad[1])\n",
    "        optimizer.apply_gradients(zip(grad,[self.w1,self.w2]))\n",
    "\n",
    "        \n",
    "            \n",
    "\n",
    "    \n",
    "            \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([3 4], shape=(2,), dtype=int32) tf.Tensor([4 1], shape=(2,), dtype=int32)\n",
      "Loss: 0.046743385\n",
      "Loss: 0.04598952\n",
      "Loss: 0.04498322\n",
      "Loss: 0.04382078\n",
      "Loss: 0.042555574\n",
      "Loss: 0.04122288\n",
      "Loss: 0.039847992\n",
      "Loss: 0.03844986\n",
      "Loss: 0.037043072\n",
      "Loss: 0.035639007\n",
      "Loss: 0.034246635\n",
      "Loss: 0.03287306\n",
      "Loss: 0.031523865\n",
      "Loss: 0.030203475\n",
      "Loss: 0.028915294\n",
      "Loss: 0.027661914\n",
      "Loss: 0.026445292\n",
      "Loss: 0.02526677\n",
      "Loss: 0.024127262\n",
      "Loss: 0.02302732\n",
      "Loss: 0.021967124\n",
      "Loss: 0.020946596\n",
      "Loss: 0.01996547\n",
      "Loss: 0.019023247\n",
      "Loss: 0.01811931\n",
      "Loss: 0.017252903\n",
      "Loss: 0.016423201\n",
      "Loss: 0.015629262\n",
      "Loss: 0.014870113\n",
      "Loss: 0.014144741\n",
      "Loss: 0.013452079\n",
      "Loss: 0.012791059\n",
      "Loss: 0.012160602\n",
      "Loss: 0.011559611\n",
      "Loss: 0.010987004\n",
      "Loss: 0.010441694\n",
      "Loss: 0.009922615\n",
      "Loss: 0.009428695\n",
      "Loss: 0.008958882\n",
      "Loss: 0.008512134\n",
      "Loss: 0.008087448\n",
      "Loss: 0.007683813\n",
      "Loss: 0.0073002577\n",
      "Loss: 0.006935841\n",
      "Loss: 0.006589642\n",
      "Loss: 0.006260787\n",
      "Loss: 0.005948431\n",
      "Loss: 0.005651758\n",
      "Loss: 0.0053699967\n",
      "Loss: 0.0051024\n",
      "Loss: 0.004848268\n",
      "Loss: 0.0046069203\n",
      "Loss: 0.0043777185\n",
      "Loss: 0.0041600526\n",
      "Loss: 0.0039533353\n",
      "Loss: 0.0037570244\n",
      "Loss: 0.0035705867\n",
      "Loss: 0.0033935201\n",
      "Loss: 0.0032253573\n",
      "Loss: 0.0030656408\n",
      "Loss: 0.002913949\n",
      "Loss: 0.002769872\n",
      "Loss: 0.0026330177\n",
      "Loss: 0.00250303\n",
      "Loss: 0.0023795518\n",
      "Loss: 0.0022622617\n",
      "Loss: 0.0021508376\n",
      "Loss: 0.0020449848\n",
      "Loss: 0.0019444211\n",
      "Loss: 0.0018488807\n",
      "Loss: 0.0017581065\n",
      "Loss: 0.0016718551\n",
      "Loss: 0.0015899028\n",
      "Loss: 0.0015120291\n",
      "Loss: 0.001438028\n",
      "Loss: 0.0013677047\n",
      "Loss: 0.001300871\n",
      "Loss: 0.0012373547\n",
      "Loss: 0.0011769858\n",
      "Loss: 0.0011196069\n",
      "Loss: 0.0010650668\n",
      "Loss: 0.0010132218\n",
      "Loss: 0.0009639394\n",
      "Loss: 0.0009170883\n",
      "Loss: 0.0008725484\n",
      "Loss: 0.00083020126\n",
      "Loss: 0.00078993576\n",
      "Loss: 0.0007516537\n",
      "Loss: 0.0007152499\n",
      "Loss: 0.0006806327\n",
      "Loss: 0.0006477127\n",
      "Loss: 0.0006164053\n",
      "Loss: 0.000586631\n",
      "Loss: 0.0005583118\n",
      "Loss: 0.0005313751\n",
      "Loss: 0.0005057529\n",
      "Loss: 0.0004813803\n",
      "Loss: 0.00045819365\n",
      "Loss: 0.0004361343\n",
      "Loss: 0.00041514906\n"
     ]
    }
   ],
   "source": [
    "            \n",
    "X=np.array(([0,0,1],[0,1,1],[1,0,1],[1,1,1]), dtype=float)\n",
    "y=np.array(([0],[1],[1],[0]), dtype=float)\n",
    "            \n",
    "NN = NeuralNetwork(X, y)\n",
    "for i in range(100):\n",
    "    NN.train(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 1]\n",
      " [1 1 1 1]\n",
      " [1 1 1 1]] [[2 2 2]\n",
      " [2 2 2]\n",
      " [2 2 2]\n",
      " [2 2 2]]\n",
      "[[8 8 8]\n",
      " [8 8 8]\n",
      " [8 8 8]]\n"
     ]
    }
   ],
   "source": [
    "# x=numpy_to_tensor(np.array(([0,0,1],[0,1,1],[1,0,1],[1,1,1]), dtype=float))\n",
    "\n",
    "x= tf.ones((3, 4))\n",
    "\n",
    "w1=numpy_to_tensor(np.random.rand(3,4))\n",
    "y=tf.ones((4, 3))*2\n",
    "\n",
    "tf.print(x, y)\n",
    "\n",
    "tf.print(tf.tensordot(x, y, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
       "array([[0.        , 0.        , 0.02300561],\n",
       "       [0.        , 0.31643727, 0.3550229 ],\n",
       "       [0.7240439 , 0.        , 0.07971293],\n",
       "       [0.36281088, 0.07566915, 0.12640192]], dtype=float32)>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.multiply(x, tf.transpose(w1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "# to read the training data and making use of vocabulary and index the chars:\n",
    "# To read the training data and make a vocabulary and dictiornary to index the chars\n",
    "class DataReader:\n",
    "    def __init__(self, path, seq_length):\n",
    "        #uncomment below , if you dont want to use any file for text reading and comment next 2 lines\n",
    "        #self.data = \"some really long text to test this. maybe not perfect but should get you going.\"\n",
    "        self.fp = open(path, \"r\")\n",
    "        self.data = self.fp.read()\n",
    "        #find unique chars\n",
    "        chars = list(set(self.data))\n",
    "        #create dictionary mapping for each char\n",
    "        self.char_to_ix = {ch:i for (i,ch) in enumerate(chars)}\n",
    "        self.ix_to_char = {i:ch for (i,ch) in enumerate(chars)}\n",
    "        #total data\n",
    "        self.data_size = len(self.data)\n",
    "        #num of unique chars\n",
    "        self.vocab_size = len(chars)\n",
    "        self.pointer = 0\n",
    "        self.seq_length = seq_length\n",
    "\n",
    "    def next_batch(self):\n",
    "        input_start = self.pointer\n",
    "        input_end = self.pointer + self.seq_length\n",
    "        inputs = [self.char_to_ix[ch] for ch in self.data[input_start:input_end]]\n",
    "        targets = [self.char_to_ix[ch] for ch in self.data[input_start+1:input_end+1]]\n",
    "        self.pointer += self.seq_length\n",
    "        if self.pointer + self.seq_length + 1 >= self.data_size:\n",
    "            # reset pointer\n",
    "            self.pointer = 0\n",
    "        return inputs, targets\n",
    "\n",
    "    def just_started(self):\n",
    "        return self.pointer == 0\n",
    "\n",
    "    def close(self):\n",
    "        self.fp.close()\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = open(\"temp.txt\", \"r\")\n",
    "data = fp.read()\n",
    "#find unique chars\n",
    "chars = list(set(data))\n",
    "#create dictionary mapping for each char\n",
    "char_to_ix = {ch:i for (i,ch) in enumerate(chars)}\n",
    "ix_to_char = {i:ch for (i,ch) in enumerate(chars)}\n",
    "#total data\n",
    "data_size = len(data)\n",
    "#num of unique chars\n",
    "vocab_size = len(chars)\n",
    "pointer = 0\n",
    "seq_length = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_params(vocab_size, num_hidden):\n",
    "    num_inputs = num_outputs = vocab_size\n",
    "\n",
    "    def normal(shape):\n",
    "        return tf.random.normal(shape=shape,stddev=0.01,mean=0,dtype=tf.float32)\n",
    "\n",
    "    # Hidden layer parameters\n",
    "    W_xh = tf.Variable(normal((num_inputs, num_hiddens)), dtype=tf.float32)\n",
    "    W_hh = tf.Variable(normal((num_hiddens, num_hiddens)), dtype=tf.float32)\n",
    "    b_h = tf.Variable(tf.zeros(num_hiddens), dtype=tf.float32)\n",
    "    # Output layer parameters\n",
    "    W_hq = tf.Variable(normal((num_hiddens, num_outputs)), dtype=tf.float32)\n",
    "    b_q = tf.Variable(tf.zeros(num_outputs), dtype=tf.float32)\n",
    "    params = [W_xh, W_hh, b_h, W_hq, b_q]\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the initial state:\n",
    "def init_rnn_state(batch_size, num_hiddens):\n",
    "    return (tf.zeros((batch_size, num_hiddens)), )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn(inputs, state, params):\n",
    "    # Here `inputs` shape: (`num_steps`, `batch_size`, `vocab_size`)\n",
    "    W_xh, W_hh, b_h, W_hq, b_q = params\n",
    "    H, = state\n",
    "    outputs = []\n",
    "    # Shape of `X`: (`batch_size`, `vocab_size`)\n",
    "    for X in inputs:\n",
    "        print('input',X.shape)\n",
    "        X = tf.reshape(X,[-1,W_xh.shape[0]])\n",
    "        H = tf.tanh(tf.matmul(X, W_xh) + tf.matmul(H, W_hh) + b_h)\n",
    "        Y = tf.matmul(H, W_hq) + b_q\n",
    "        print('output', Y)\n",
    "        outputs.append(tf.math.argmax(Y[0]))\n",
    "#     print('output', outputs.shape)\n",
    "    return tf.concat(outputs, axis=0), (H,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the RNN layer CRUX:\n",
    "\n",
    "class RNNModelScratch: #@save\n",
    "    \"\"\"A RNN Model implemented from scratch.\"\"\"\n",
    "    def __init__(self, vocab_size, num_hiddens,\n",
    "                 init_state, forward_fn):\n",
    "        self.vocab_size, self.num_hiddens = vocab_size, num_hiddens\n",
    "        self.init_state, self.forward_fn = init_state, forward_fn\n",
    "\n",
    "    def __call__(self, X, state, params):\n",
    "        X = tf.one_hot(tf.transpose(X), self.vocab_size)\n",
    "        X = tf.cast(X, tf.float32)\n",
    "        return self.forward_fn(X, state, params)\n",
    "\n",
    "    def begin_state(self, batch_size):\n",
    "        return self.init_state(batch_size, self.num_hiddens)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input (23,)\n",
      "output tf.Tensor(\n",
      "[[-1.2685248e-03 -2.1264921e-03  9.7412511e-04  1.6114863e-03\n",
      "  -4.2205822e-04 -2.5995141e-03 -1.8405751e-03  6.9634756e-05\n",
      "   2.9907308e-03 -2.7920017e-03 -1.6235260e-03 -1.5650267e-03\n",
      "   1.9201986e-03  1.8325539e-03  9.4695447e-04  3.6256784e-04\n",
      "  -2.2264235e-03  5.1837876e-03  3.0615844e-03  1.3767412e-03\n",
      "  -2.0205849e-04 -1.8630987e-03 -1.7162709e-03]\n",
      " [-1.2685248e-03 -2.1264921e-03  9.7412511e-04  1.6114863e-03\n",
      "  -4.2205822e-04 -2.5995141e-03 -1.8405751e-03  6.9634756e-05\n",
      "   2.9907308e-03 -2.7920017e-03 -1.6235260e-03 -1.5650267e-03\n",
      "   1.9201986e-03  1.8325539e-03  9.4695447e-04  3.6256784e-04\n",
      "  -2.2264235e-03  5.1837876e-03  3.0615844e-03  1.3767412e-03\n",
      "  -2.0205849e-04 -1.8630987e-03 -1.7162709e-03]\n",
      " [-1.2685248e-03 -2.1264921e-03  9.7412511e-04  1.6114863e-03\n",
      "  -4.2205822e-04 -2.5995141e-03 -1.8405751e-03  6.9634756e-05\n",
      "   2.9907308e-03 -2.7920017e-03 -1.6235260e-03 -1.5650267e-03\n",
      "   1.9201986e-03  1.8325539e-03  9.4695447e-04  3.6256784e-04\n",
      "  -2.2264235e-03  5.1837876e-03  3.0615844e-03  1.3767412e-03\n",
      "  -2.0205849e-04 -1.8630987e-03 -1.7162709e-03]\n",
      " [-1.2685248e-03 -2.1264921e-03  9.7412511e-04  1.6114863e-03\n",
      "  -4.2205822e-04 -2.5995141e-03 -1.8405751e-03  6.9634756e-05\n",
      "   2.9907308e-03 -2.7920017e-03 -1.6235260e-03 -1.5650267e-03\n",
      "   1.9201986e-03  1.8325539e-03  9.4695447e-04  3.6256784e-04\n",
      "  -2.2264235e-03  5.1837876e-03  3.0615844e-03  1.3767412e-03\n",
      "  -2.0205849e-04 -1.8630987e-03 -1.7162709e-03]\n",
      " [-1.2685248e-03 -2.1264921e-03  9.7412511e-04  1.6114863e-03\n",
      "  -4.2205822e-04 -2.5995141e-03 -1.8405751e-03  6.9634756e-05\n",
      "   2.9907308e-03 -2.7920017e-03 -1.6235260e-03 -1.5650267e-03\n",
      "   1.9201986e-03  1.8325539e-03  9.4695447e-04  3.6256784e-04\n",
      "  -2.2264235e-03  5.1837876e-03  3.0615844e-03  1.3767412e-03\n",
      "  -2.0205849e-04 -1.8630987e-03 -1.7162709e-03]], shape=(5, 23), dtype=float32)\n",
      "input (23,)\n",
      "output tf.Tensor(\n",
      "[[ 1.71583565e-03  1.66310102e-03  1.29490835e-03  8.71957629e-04\n",
      "  -1.37407950e-03  3.83202429e-03 -3.47426208e-03 -1.11648114e-04\n",
      "   2.00745021e-03 -2.12402316e-04  1.21040607e-03  5.35693951e-04\n",
      "   2.20819237e-03 -2.52464809e-03 -4.79458598e-04  1.32483197e-03\n",
      "  -6.78258832e-04 -4.16865194e-04 -9.38485609e-05 -7.76679837e-04\n",
      "  -4.19145945e-04 -2.34082690e-03  2.01028842e-03]\n",
      " [ 1.71583565e-03  1.66310102e-03  1.29490835e-03  8.71957629e-04\n",
      "  -1.37407950e-03  3.83202429e-03 -3.47426208e-03 -1.11648114e-04\n",
      "   2.00745021e-03 -2.12402316e-04  1.21040607e-03  5.35693951e-04\n",
      "   2.20819237e-03 -2.52464809e-03 -4.79458598e-04  1.32483197e-03\n",
      "  -6.78258832e-04 -4.16865194e-04 -9.38485609e-05 -7.76679837e-04\n",
      "  -4.19145945e-04 -2.34082690e-03  2.01028842e-03]\n",
      " [ 1.71583565e-03  1.66310102e-03  1.29490835e-03  8.71957629e-04\n",
      "  -1.37407950e-03  3.83202429e-03 -3.47426208e-03 -1.11648114e-04\n",
      "   2.00745021e-03 -2.12402316e-04  1.21040607e-03  5.35693951e-04\n",
      "   2.20819237e-03 -2.52464809e-03 -4.79458598e-04  1.32483197e-03\n",
      "  -6.78258832e-04 -4.16865194e-04 -9.38485609e-05 -7.76679837e-04\n",
      "  -4.19145945e-04 -2.34082690e-03  2.01028842e-03]\n",
      " [ 1.71583565e-03  1.66310102e-03  1.29490835e-03  8.71957629e-04\n",
      "  -1.37407950e-03  3.83202429e-03 -3.47426208e-03 -1.11648114e-04\n",
      "   2.00745021e-03 -2.12402316e-04  1.21040607e-03  5.35693951e-04\n",
      "   2.20819237e-03 -2.52464809e-03 -4.79458598e-04  1.32483197e-03\n",
      "  -6.78258832e-04 -4.16865194e-04 -9.38485609e-05 -7.76679837e-04\n",
      "  -4.19145945e-04 -2.34082690e-03  2.01028842e-03]\n",
      " [ 1.71583565e-03  1.66310102e-03  1.29490835e-03  8.71957629e-04\n",
      "  -1.37407950e-03  3.83202429e-03 -3.47426208e-03 -1.11648114e-04\n",
      "   2.00745021e-03 -2.12402316e-04  1.21040607e-03  5.35693951e-04\n",
      "   2.20819237e-03 -2.52464809e-03 -4.79458598e-04  1.32483197e-03\n",
      "  -6.78258832e-04 -4.16865194e-04 -9.38485609e-05 -7.76679837e-04\n",
      "  -4.19145945e-04 -2.34082690e-03  2.01028842e-03]], shape=(5, 23), dtype=float32)\n",
      "input (23,)\n",
      "output tf.Tensor(\n",
      "[[-0.00105442  0.00029264  0.00081999  0.00240408 -0.00305724  0.00071701\n",
      "   0.00108497 -0.00113686  0.00023571  0.00544197 -0.00067117  0.00166396\n",
      "  -0.00265333 -0.00061306 -0.00236732  0.00046601  0.00136819 -0.00122404\n",
      "  -0.00107386  0.00395416  0.00045202 -0.00049251 -0.00249497]\n",
      " [-0.00105442  0.00029264  0.00081999  0.00240408 -0.00305724  0.00071701\n",
      "   0.00108497 -0.00113686  0.00023571  0.00544197 -0.00067117  0.00166396\n",
      "  -0.00265333 -0.00061306 -0.00236732  0.00046601  0.00136819 -0.00122404\n",
      "  -0.00107386  0.00395416  0.00045202 -0.00049251 -0.00249497]\n",
      " [-0.00105442  0.00029264  0.00081999  0.00240408 -0.00305724  0.00071701\n",
      "   0.00108497 -0.00113686  0.00023571  0.00544197 -0.00067117  0.00166396\n",
      "  -0.00265333 -0.00061306 -0.00236732  0.00046601  0.00136819 -0.00122404\n",
      "  -0.00107386  0.00395416  0.00045202 -0.00049251 -0.00249497]\n",
      " [-0.00105442  0.00029264  0.00081999  0.00240408 -0.00305724  0.00071701\n",
      "   0.00108497 -0.00113686  0.00023571  0.00544197 -0.00067117  0.00166396\n",
      "  -0.00265333 -0.00061306 -0.00236732  0.00046601  0.00136819 -0.00122404\n",
      "  -0.00107386  0.00395416  0.00045202 -0.00049251 -0.00249497]\n",
      " [-0.00105442  0.00029264  0.00081999  0.00240408 -0.00305724  0.00071701\n",
      "   0.00108497 -0.00113686  0.00023571  0.00544197 -0.00067117  0.00166396\n",
      "  -0.00265333 -0.00061306 -0.00236732  0.00046601  0.00136819 -0.00122404\n",
      "  -0.00107386  0.00395416  0.00045202 -0.00049251 -0.00249497]], shape=(5, 23), dtype=float32)\n",
      "input (23,)\n",
      "output tf.Tensor(\n",
      "[[ 0.0041131   0.00199     0.0034486  -0.00417053 -0.00381383  0.00177691\n",
      "  -0.00549361  0.00552123  0.00032823  0.00084438  0.00183903 -0.00052194\n",
      "   0.00213796 -0.00129133  0.00377389 -0.00354147 -0.00210106 -0.00224746\n",
      "  -0.00146704 -0.00165589  0.00030938  0.00141462 -0.00030786]\n",
      " [ 0.0041131   0.00199     0.0034486  -0.00417053 -0.00381383  0.00177691\n",
      "  -0.00549361  0.00552123  0.00032823  0.00084438  0.00183903 -0.00052194\n",
      "   0.00213796 -0.00129133  0.00377389 -0.00354147 -0.00210106 -0.00224746\n",
      "  -0.00146704 -0.00165589  0.00030938  0.00141462 -0.00030786]\n",
      " [ 0.0041131   0.00199     0.0034486  -0.00417053 -0.00381383  0.00177691\n",
      "  -0.00549361  0.00552123  0.00032823  0.00084438  0.00183903 -0.00052194\n",
      "   0.00213796 -0.00129133  0.00377389 -0.00354147 -0.00210106 -0.00224746\n",
      "  -0.00146704 -0.00165589  0.00030938  0.00141462 -0.00030786]\n",
      " [ 0.0041131   0.00199     0.0034486  -0.00417053 -0.00381383  0.00177691\n",
      "  -0.00549361  0.00552123  0.00032823  0.00084438  0.00183903 -0.00052194\n",
      "   0.00213796 -0.00129133  0.00377389 -0.00354147 -0.00210106 -0.00224746\n",
      "  -0.00146704 -0.00165589  0.00030938  0.00141462 -0.00030786]\n",
      " [ 0.0041131   0.00199     0.0034486  -0.00417053 -0.00381383  0.00177691\n",
      "  -0.00549361  0.00552123  0.00032823  0.00084438  0.00183903 -0.00052194\n",
      "   0.00213796 -0.00129133  0.00377389 -0.00354147 -0.00210106 -0.00224746\n",
      "  -0.00146704 -0.00165589  0.00030938  0.00141462 -0.00030786]], shape=(5, 23), dtype=float32)\n",
      "input (23,)\n",
      "output tf.Tensor(\n",
      "[[ 1.0082785e-03  3.7415698e-03  7.1441429e-04  1.3053555e-03\n",
      "   2.9767463e-03 -5.0841244e-03 -2.8986800e-03  3.1498265e-03\n",
      "   1.3729795e-03 -2.5965122e-03 -1.3797863e-03 -3.1906278e-03\n",
      "  -1.2728474e-03  4.1602179e-05  2.8580264e-04 -3.2137115e-03\n",
      "  -5.9739910e-03 -1.0742398e-03  2.0277940e-03 -2.4534399e-03\n",
      "   2.7313508e-04 -2.9667909e-03 -1.8302093e-03]\n",
      " [ 1.0082785e-03  3.7415698e-03  7.1441429e-04  1.3053555e-03\n",
      "   2.9767463e-03 -5.0841244e-03 -2.8986800e-03  3.1498265e-03\n",
      "   1.3729795e-03 -2.5965122e-03 -1.3797863e-03 -3.1906278e-03\n",
      "  -1.2728474e-03  4.1602179e-05  2.8580264e-04 -3.2137115e-03\n",
      "  -5.9739910e-03 -1.0742398e-03  2.0277940e-03 -2.4534399e-03\n",
      "   2.7313508e-04 -2.9667909e-03 -1.8302093e-03]\n",
      " [ 1.0082785e-03  3.7415698e-03  7.1441429e-04  1.3053555e-03\n",
      "   2.9767463e-03 -5.0841244e-03 -2.8986800e-03  3.1498265e-03\n",
      "   1.3729795e-03 -2.5965122e-03 -1.3797863e-03 -3.1906278e-03\n",
      "  -1.2728474e-03  4.1602179e-05  2.8580264e-04 -3.2137115e-03\n",
      "  -5.9739910e-03 -1.0742398e-03  2.0277940e-03 -2.4534399e-03\n",
      "   2.7313508e-04 -2.9667909e-03 -1.8302093e-03]\n",
      " [ 1.0082785e-03  3.7415698e-03  7.1441429e-04  1.3053555e-03\n",
      "   2.9767463e-03 -5.0841244e-03 -2.8986800e-03  3.1498265e-03\n",
      "   1.3729795e-03 -2.5965122e-03 -1.3797863e-03 -3.1906278e-03\n",
      "  -1.2728474e-03  4.1602179e-05  2.8580264e-04 -3.2137115e-03\n",
      "  -5.9739910e-03 -1.0742398e-03  2.0277940e-03 -2.4534399e-03\n",
      "   2.7313508e-04 -2.9667909e-03 -1.8302093e-03]\n",
      " [ 1.0082785e-03  3.7415698e-03  7.1441429e-04  1.3053555e-03\n",
      "   2.9767463e-03 -5.0841244e-03 -2.8986800e-03  3.1498265e-03\n",
      "   1.3729795e-03 -2.5965122e-03 -1.3797863e-03 -3.1906278e-03\n",
      "  -1.2728474e-03  4.1602179e-05  2.8580264e-04 -3.2137115e-03\n",
      "  -5.9739910e-03 -1.0742398e-03  2.0277940e-03 -2.4534399e-03\n",
      "   2.7313508e-04 -2.9667909e-03 -1.8302093e-03]], shape=(5, 23), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(TensorShape([5]), 1, TensorShape([5, 512]))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_hiddens=512\n",
    "seq_len=5\n",
    "X= tf.Variable([char_to_ix[ch] for ch in data[0:5]])\n",
    "model=RNNModelScratch(23, num_hiddens, init_rnn_state, rnn)\n",
    "\n",
    "state = model.begin_state(X.shape[0])\n",
    "params = get_params(23, num_hiddens)\n",
    "Y, new_state = model(X, state, params)\n",
    "Y.shape, len(new_state), new_state[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(23,), dtype=float32, numpy=\n",
       "array([ 3.5496957e-03, -1.5880831e-03, -8.1366283e-04, -7.4636005e-04,\n",
       "       -2.1743190e-03, -2.1476911e-03, -1.6094119e-03,  1.0846057e-03,\n",
       "       -6.8735047e-03,  9.2799775e-04,  5.2702031e-05, -1.5214975e-03,\n",
       "        2.2887054e-04,  5.2304380e-04, -3.5918376e-04,  2.3214282e-03,\n",
       "       -2.1889429e-03, -4.2013042e-03,  6.2897464e-04,  1.5773850e-03,\n",
       "        2.7675428e-03, -7.9500768e-04, -2.6871930e-03], dtype=float32)>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "something=tf.math.argmax(Y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "tf.print(something)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
